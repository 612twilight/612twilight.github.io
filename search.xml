<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>吉他谱_祝你生日快乐</title>
      <link href="/2018/11/10/%E5%90%89%E4%BB%96%E8%B0%B1-%E7%A5%9D%E4%BD%A0%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/"/>
      <url>/2018/11/10/%E5%90%89%E4%BB%96%E8%B0%B1-%E7%A5%9D%E4%BD%A0%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/</url>
      
        <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要="">摘要：这是一篇生日快乐吉他谱<a id="more"></a></excerpt></p><the rest="" of="" contents="" |="" 余下全文=""><p>弦从下往上数，0品为空弦</p><p>弦    3    3    3    3    2    2</p><p>品    0    0    2    0    1    0</p><p>​    祝    你    生    日    快    乐</p><p>弦    3    3    3    3    2    2</p><p>品    0    0    2    0    1    0</p><p>​    祝    你    生    日    快    乐</p><p>弦    3    3    1    1    2    2    3</p><p>品    0    0    3    0    1    0    2</p><p>​    祝    你    生    日    快    乐    ~</p><p>弦    1    1    1    2    2    2</p><p>品    1    1    0    1    3    1</p><p>​    祝    你    生    日    快    乐</p></the>]]></content>
      
      
      <categories>
          
          <category> 吉他谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生日快乐 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python里的Data streaming总结</title>
      <link href="/2018/11/10/python%E9%87%8C%E7%9A%84Data-streaming%E6%80%BB%E7%BB%93/"/>
      <url>/2018/11/10/python%E9%87%8C%E7%9A%84Data-streaming%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要="">摘要：这是一篇介绍python里的data streaming的文章，关于generators, iterators, iterables都有涉及到<a id="more"></a></excerpt></p><the rest="" of="" contents="" |="" 余下全文=""><h1 id="最重要的是节约了内存"><a href="#最重要的是节约了内存" class="headerlink" title="最重要的是节约了内存"></a>最重要的是节约了内存</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">numbers = range(100000)</span><br><span class="line"> </span><br><span class="line">sum([n**2 for n in numbers])</span><br><span class="line">333328333350000</span><br><span class="line"> </span><br><span class="line"># Generator: square and sum one value after another</span><br><span class="line"># No extra array created = lazily evaluated stream of numbers!</span><br><span class="line">sum(n**2 for n in numbers)</span><br><span class="line">333328333350000</span><br></pre></td></tr></table></figure><p>第一种方法没有使用streaming的方式，他首先创造了一个list，然后对它求和，是很浪费的。</p><p>第二种方法则是对一个元平方求和之后再处理下一个元素，没有创建一个list</p><p>这两种方法也许在时间效率上有所区别，但是在任何一个严格的数据处理过程中，语言的开销远远小于数据生成和计算的开销，迭代序列的真正力量在于<strong>节约内存</strong>！</p><h1 id="Generators-iterators-iterables的区别"><a href="#Generators-iterators-iterables的区别" class="headerlink" title="Generators, iterators, iterables的区别"></a>Generators, iterators, iterables的区别</h1><p>iterator是我们最终关心的，它是实际管理一个序列里的单个元素的对象。而iterables和generators则是可以提供一个iterator的机制。这两者也是也是有区别的。</p><p>generator只能提供一次iterator，也就是你遍历一遍generator，那数据就已经消耗完了，在运行它也不会有更多的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">generator = (word + &apos;!&apos; for word in &apos;baby let me iterate ya&apos;.split())</span><br><span class="line"># 一个generator对象创建好了，等待被迭代</span><br><span class="line"># 在这个点，惊叹号还没有添加进去</span><br><span class="line"> </span><br><span class="line">for val in generator: # 在迭代过程中，真正的处理过程在这里，这时候惊叹号才产生</span><br><span class="line">    print val,</span><br><span class="line"># baby! let! me! iterate! ya!</span><br><span class="line"> </span><br><span class="line">for val in generator:</span><br><span class="line">    print val,</span><br><span class="line"># 什么都不会打印出来，没有数据了，generator数据流之前已经消耗完了</span><br></pre></td></tr></table></figure><p>iterable则不一样，只要当每次循环完成，就可以产生一个新的iterator（具体的说，是<em>iterable._ iter_ ()</em>被调用，例如python里面每次调用for循环）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class BeyonceIterable(object):</span><br><span class="line">    def __iter__(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        The iterable interface: return an iterator from __iter__().</span><br><span class="line">        Every generator is an iterator implicitly (but not vice versa!),</span><br><span class="line">        so implementing `__iter__` as a generator is the easiest way to create streamed iterables.</span><br><span class="line">        iterable的接口，会通过__iter__()返回一个iterator</span><br><span class="line">        每个generator都可以看做一个隐含的iterator，所以实现一个generator是创建一个可迭代对象的最简单的方式</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        for word in &apos;baby let me iterate ya&apos;.split():</span><br><span class="line">            yield word + &apos;!&apos;  # 这里使用的是yield去创建generator，和之前的本质是一样的。uses yield =&gt; __iter__ is a generator</span><br><span class="line"> </span><br><span class="line">iterable = BeyonceIterable()</span><br><span class="line"> </span><br><span class="line">for val in iterable:  # 创造了一个iterator</span><br><span class="line">    print val,</span><br><span class="line"># baby! let! me! iterate! ya!</span><br><span class="line"> </span><br><span class="line">for val in iterable:  # 创造了另一个iterator</span><br><span class="line">    print val,</span><br><span class="line"># baby! let! me! iterate! ya!</span><br></pre></td></tr></table></figure><p>一般而言，iterables是一个更有用的机制，他可以允许你多次读取某一个序列。当然，如果你的数据只能被读取一次的话，那就只能使用generator。</p></the>]]></content>
      
      
      <categories>
          
          <category> python技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> data streaming </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>jieba之分词</title>
      <link href="/2018/11/09/%E5%88%86%E8%AF%8D/"/>
      <url>/2018/11/09/%E5%88%86%E8%AF%8D/</url>
      
        <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要="">摘要：这是一篇介绍如何使用jieba去分词的博客<a id="more"></a></excerpt></p><the rest="" of="" contents="" |="" 余下全文=""><h1 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br></pre></td></tr></table></figure><h1 id="建立短句"><a href="#建立短句" class="headerlink" title="建立短句"></a>建立短句</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s = u&apos;我想和女朋友一起去北京故宫博物院参观和闲逛。&apos;</span><br></pre></td></tr></table></figure><p>这里的u是告诉python是使用Unicode编码解读里面的数据，一般在文件开头加上# -<em>- coding: UTF-8 -</em>-也可以做到</p><h1 id="精确模式"><a href="#精确模式" class="headerlink" title="精确模式"></a>精确模式</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cut = jieba.cut(s)</span><br><span class="line">print(&quot;【精确模式下输出】&quot;)</span><br><span class="line">print(cut) </span><br><span class="line"># &lt;generator object Tokenizer.cut at 0x0000023490DA3888&gt;</span><br><span class="line"># 这里输出的是一个generator</span><br><span class="line">print(&apos;,&apos;.join(cut))  </span><br><span class="line"># &apos;,&apos;.join(cut)表示使用&quot;,&quot;将cut里面的序列连接起来</span><br><span class="line"># 我,想,和,女朋友,一起,去,北京故宫博物院,参观,和,闲逛,。</span><br></pre></td></tr></table></figure><h1 id="全局模式"><a href="#全局模式" class="headerlink" title="全局模式"></a>全局模式</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cutall=jieba.cut(s,cut_all = True)</span><br><span class="line">print(&quot;【全局模式下输出】&quot;)</span><br><span class="line">print(&apos;,&apos;.join(cutall))</span><br><span class="line"># 我,想,和,女朋友,朋友,一起,去,北京,北京故宫,北京故宫博物院,故宫,故宫博物院,博物,博物院,参观,和,闲逛,,</span><br></pre></td></tr></table></figure><p>全局模式下会尽可能多的对字符串进行切分</p><h1 id="搜索引擎模式"><a href="#搜索引擎模式" class="headerlink" title="搜索引擎模式"></a>搜索引擎模式</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cutsearch = jieba.cut_for_search(s)</span><br><span class="line">print(&apos;【搜索引擎模式下输出】&apos;)</span><br><span class="line">print(&apos;,&apos;.join(cutsearch))</span><br><span class="line"># 我,想,和,朋友,女朋友,一起,去,北京,故宫,博物,博物院,北京故宫博物院,参观,和,闲逛,。</span><br></pre></td></tr></table></figure><p>在精确模式的基础上，对长词在此划分</p><h1 id="使用lcut获取list"><a href="#使用lcut获取list" class="headerlink" title="使用lcut获取list"></a>使用lcut获取list</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cut_s = jieba.lcut(s)</span><br><span class="line">print(type(cut_s)) </span><br><span class="line"># &lt;class &apos;list&apos;&gt; 这里返回的是list</span><br><span class="line">print(cut_s)</span><br><span class="line"># [&apos;我&apos;, &apos;想&apos;, &apos;和&apos;, &apos;女朋友&apos;, &apos;一起&apos;, &apos;去&apos;, &apos;北京故宫博物院&apos;, &apos;参观&apos;, &apos;和&apos;, &apos;闲逛&apos;, &apos;。&apos;]</span><br><span class="line"></span><br><span class="line">lcut_for_search = jieba.lcut_for_search(s) # 这里也是lcut</span><br><span class="line">print(lcut_for_search)  # 返回list</span><br><span class="line"># [&apos;我&apos;, &apos;想&apos;, &apos;和&apos;, &apos;朋友&apos;, &apos;女朋友&apos;, &apos;一起&apos;, &apos;去&apos;, &apos;北京&apos;, &apos;故宫&apos;, &apos;博物&apos;, &apos;博物院&apos;, &apos;北京故宫博物院&apos;, &apos;参观&apos;, &apos;和&apos;, &apos;闲逛&apos;, &apos;。&apos;]</span><br></pre></td></tr></table></figure><h1 id="综合上述代码"><a href="#综合上述代码" class="headerlink" title="综合上述代码"></a>综合上述代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: UTF-8 -*-</span><br><span class="line">import numpy as np</span><br><span class="line"># import tensorflow as tf</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">Created by gaoyw on 2018/11/9</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import jieba</span><br><span class="line">s = &apos;我想和女朋友一起去北京故宫博物院参观和闲逛。&apos;</span><br><span class="line">cut = jieba.cut(s)</span><br><span class="line"></span><br><span class="line">print(&apos;【精确模式下输出】&apos;)</span><br><span class="line">print(cut)</span><br><span class="line">print(&apos;,&apos;.join(cut))</span><br><span class="line"></span><br><span class="line">cutall=jieba.cut(s,cut_all = True)</span><br><span class="line">print(&quot;【全局模式下输出】&quot;)</span><br><span class="line">print(&apos;,&apos;.join(cutall))</span><br><span class="line"></span><br><span class="line">cutsearch = jieba.cut_for_search(s)</span><br><span class="line">print(&apos;【搜索引擎模式下输出】&apos;)</span><br><span class="line">print(&apos;,&apos;.join(cutsearch))</span><br><span class="line"></span><br><span class="line">cut_s = jieba.lcut(s)</span><br><span class="line">print(type(cut_s))</span><br><span class="line">print(cut_s)</span><br><span class="line"></span><br><span class="line">lcut_for_search = jieba.lcut_for_search(s)</span><br><span class="line">print(lcut_for_search)</span><br></pre></td></tr></table></figure></the>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分词 </tag>
            
            <tag> jieba </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>genism之tfidf</title>
      <link href="/2018/11/09/tfidf/"/>
      <url>/2018/11/09/tfidf/</url>
      
        <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要="">摘要：这是一篇介绍如何使用genism去计算一个词在文档语料库的tfidf值的博客<a id="more"></a></excerpt></p><the rest="" of="" contents="" |="" 余下全文=""><h1 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models, similarities</span><br></pre></td></tr></table></figure><h1 id="定义语料库"><a href="#定义语料库" class="headerlink" title="定义语料库"></a>定义语料库</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],</span><br><span class="line">            [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],</span><br><span class="line">            [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],</span><br><span class="line">            [(0, 1.0), (4, 2.0), (7, 1.0)],</span><br><span class="line">            [(3, 1.0), (5, 1.0), (6, 1.0)],</span><br><span class="line">            [(9, 1.0)],</span><br><span class="line">            [(9, 1.0), (10, 1.0)],</span><br><span class="line">            [(9, 1.0), (10, 1.0), (11, 1.0)],</span><br><span class="line">            [(8, 1.0), (10, 1.0), (11, 1.0)]]</span><br></pre></td></tr></table></figure><p>corpus就是我们的文档库，其中article0=corpus[0]=[(0, 1.0), (1, 1.0), (2, 1.0)]表示的是一篇文档的组成,article0[0]=（0,1.0）表示的是单词编号为0的词在该文章里有出现过1.0次，其余组成也是这样：单词编号为1的词在该文章里出现1.0次，单词编号为2的词在该文章里出现了1.0次。</p><h1 id="建立tfidf的模型"><a href="#建立tfidf的模型" class="headerlink" title="建立tfidf的模型"></a>建立tfidf的模型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tfidf = models.TfidfModel(corpus)</span><br></pre></td></tr></table></figure><h1 id="建立需要提取的文档"><a href="#建立需要提取的文档" class="headerlink" title="建立需要提取的文档"></a>建立需要提取的文档</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_article = [(0, 1), (4, 1)]</span><br></pre></td></tr></table></figure><p>显然文档也是由向量组成的。</p><h1 id="计算新文档的tfidf的值"><a href="#计算新文档的tfidf的值" class="headerlink" title="计算新文档的tfidf的值"></a>计算新文档的tfidf的值</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result=tfidf[new_article]</span><br></pre></td></tr></table></figure><p>result就是这篇文章中所有词的tfidf值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>打印结果是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(0, 0.8075244024440723), (4, 0.5898341626740045)]</span><br></pre></td></tr></table></figure><p>结果表示的是编号为0的词的tfidf的值为0.8075244024440723，而编号为4的词的tfidf的值为0.5898341626740045</p><h1 id="综合上述代码"><a href="#综合上述代码" class="headerlink" title="综合上述代码"></a>综合上述代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models, similarities</span><br><span class="line">corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],</span><br><span class="line">            [(2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (8, 1.0)],</span><br><span class="line">            [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],</span><br><span class="line">            [(0, 1.0), (4, 2.0), (7, 1.0)],</span><br><span class="line">            [(3, 1.0), (5, 1.0), (6, 1.0)],</span><br><span class="line">            [(9, 1.0)],</span><br><span class="line">            [(9, 1.0), (10, 1.0)],</span><br><span class="line">            [(9, 1.0), (10, 1.0), (11, 1.0)],</span><br><span class="line">            [(8, 1.0), (10, 1.0), (11, 1.0)]]</span><br><span class="line">print(np.shape(corpus))</span><br><span class="line">tfidf = models.TfidfModel(corpus)</span><br><span class="line">new_article = [(0, 1), (4, 1)]</span><br><span class="line">result = tfidf[new_article]</span><br><span class="line">print(result) </span><br><span class="line"># [(0, 0.8075244024440723), (4, 0.5898341626740045)]</span><br></pre></td></tr></table></figure></the>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gensim </tag>
            
            <tag> tfidf </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>word2vec工具探索</title>
      <link href="/2018/11/09/word2vec/"/>
      <url>/2018/11/09/word2vec/</url>
      
        <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要="">摘要：这是一篇介绍如何使用genism去计算一个词的word embedding<a id="more"></a></excerpt></p><the rest="" of="" contents="" |="" 余下全文=""><h1 id="初识"><a href="#初识" class="headerlink" title="初识"></a>初识</h1><p>这是最简单的使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import gensim</span><br><span class="line"></span><br><span class="line">sentences = [[&apos;first&apos;, &apos;sentence&apos;], [&apos;second&apos;, &apos;sentence&apos;]]</span><br><span class="line"># train word2vec on the two sentences</span><br><span class="line">model = gensim.models.Word2Vec(sentences, size=5, min_count=1)</span><br><span class="line">word_vec = model[&apos;first&apos;]</span><br><span class="line">print(word_vec)</span><br><span class="line"># [ 0.03712391 -0.01817003 -0.06288775 -0.04320185 -0.05464336]</span><br></pre></td></tr></table></figure><h1 id="模型参数介绍"><a href="#模型参数介绍" class="headerlink" title="模型参数介绍"></a>模型参数介绍</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=&lt;built-in function hash&gt;, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)</span><br></pre></td></tr></table></figure><p>sentence：用于训练的数据，The sentences iterable can be simply a list of lists of tokens ，意思是它是list of lists，最里层的list就是句子，sentences = [[‘first’, ‘sentence’], [‘second’, ‘sentence’]]就是表示第一个句子是first sentence，第二个句子是second sentence。但是他也可以表示为<em>iterable of iterables</em> ,iterable的一些介绍可以看我的另一篇博文，简单来说就是可以多次生成iterator的对象。</p><p>corpus_file: 这是一个用来加速的选项，需要用genism里面的一个处理函数，没见人用过</p><p>size：(<em>int,</em> <em>optional</em>)  - 是指特征向量的维度，默认为100 </p><p>window：(<em>int,</em> <em>optional</em>) —表示当前词与预测词在一个句子中的最大距离是多少 </p><p>alpha: (<em>float,</em> <em>optional</em>) - 是学习速率 </p><p>min_alpha：<em>(</em>float<em>,</em> <em>optional</em>) – 学习率会线性的下降到min_alpha的值</p><p>seed：(<em>int,</em> <em>optional</em>)  - 用于随机数发生器。与初始化词向量有关。 </p><p>sg： (<em>{0</em>, <em>1},</em> <em>optional</em>) –  1 表示使用skip-gram; 其他表示使用CBOW.</p><p>hs：(<em>{0,</em> <em>1},</em> <em>optional</em>) – 如果是1, 表示使用hierarchical softmax去训练. 如果是0, 并且negative参数非零,会使用负采样训练。</p><p>negative： (<em>int,</em> <em>optional</em>) – 这里说的是负采样会加入几个噪声进去，表示负的程度，一般取5到20个词作为负采样，如果设置成0，那么就不会有负样本</p><p>cbow_mean:  (<em>{0,</em> <em>1},</em> <em>optional</em>) - 如果为0，则采用上下文词向量的和，如果为1（defau·t）则采用均值。只有使用CBOW的时候才起作用。</p><p>min_count :  (<em>int,</em> <em>optional</em>) - 词频少于min_count次数的单词会被丢弃掉, 默认值为5</p><p>max_vocab_size: (<em>int,</em> <em>optional</em>) - 设置词向量构建期间的RAM限制。如果所有独立单词个数超过这个，则就消除掉其中最不频繁的一个。每一千万个单词需要大约1GB的RAM。设置成None则没有限制。</p><p>sample:  (<em>float,</em> <em>optional</em>) - 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)</p><p>workers： (<em>int,</em> <em>optional</em>)  - 参数控制训练的并行数。</p><p>hashfxn： hash函数来初始化权重。默认使用python的hash函数</p><p>iter：(<em>int,</em> <em>optional</em>)  - 迭代次数，默认为5，一般会有默认的第一次，用来构建字典树</p><p>trim_rule： (<em>function,</em> <em>optional</em>)  用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）或者一个接受()并返回RU·E_DISCARD,uti·s.RU·E_KEEP或者uti·s.RU·E_DEFAU·T的函数。</p><p>sorted_vocab： (<em>{0,</em> <em>1},</em> <em>optional</em>) – 如果为1（default），则在分配word index 的时候会先对单词基于频率降序排序。</p><p>batch_words：(<em>int,</em> <em>optional</em>) - 每一批的传递给线程的单词的数量，默认为10000</p><h1 id="使用iterable传入对象"><a href="#使用iterable传入对象" class="headerlink" title="使用iterable传入对象"></a>使用iterable传入对象</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class MySentences(object):</span><br><span class="line">    def __init__(self, dirname):</span><br><span class="line">        self.dirname = dirname</span><br><span class="line"> </span><br><span class="line">    def __iter__(self):</span><br><span class="line">        for fname in os.listdir(self.dirname):</span><br><span class="line">            for line in open(os.path.join(self.dirname, fname)):</span><br><span class="line">                yield line.split()</span><br><span class="line"> </span><br><span class="line">sentences = MySentences(&apos;/some/directory&apos;) # a memory-friendly iterator</span><br><span class="line">model = gensim.models.Word2Vec(sentences)</span><br></pre></td></tr></table></figure><p>这里的sentence是一个可迭代对象，每次给模型的是一个list（文件夹里面文件的某一行的单词list），然后逐步把所有语料库都传入进去，这样可以不用一次加载全部语料库，节约了内存</p><h1 id="输入流无法重复利用"><a href="#输入流无法重复利用" class="headerlink" title="输入流无法重复利用"></a>输入流无法重复利用</h1><p>有时候我们的输入流是无法重复使用的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model = gensim.models.Word2Vec(iter=1)  </span><br><span class="line"># 建立一个空模型 an empty model, no training yet</span><br><span class="line">model.build_vocab(some_sentences) </span><br><span class="line"># 先建立字典树 不要使用训练的数据，用别的已知的数据</span><br><span class="line"># can be a non-repeatable, 1-pass generator</span><br><span class="line">model.train(other_sentences)  </span><br><span class="line"># 使用这次训练的语料库进行训练 </span><br><span class="line"># can be a non-repeatable, 1-pass generator</span><br></pre></td></tr></table></figure><h1 id="存储模型"><a href="#存储模型" class="headerlink" title="存储模型"></a>存储模型</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.save(&apos;/tmp/mymodel&apos;)</span><br><span class="line">new_model = gensim.models.Word2Vec.load(&apos;/tmp/mymodel&apos;)</span><br></pre></td></tr></table></figure><h1 id="载入模型并进一步训练"><a href="#载入模型并进一步训练" class="headerlink" title="载入模型并进一步训练"></a>载入模型并进一步训练</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = gensim.models.Word2Vec.load(&apos;/tmp/mymodel&apos;)</span><br><span class="line">more_sentence = [[&quot;some&quot;,&quot;more&quot;,&quot;sentences&quot;]]</span><br><span class="line">model.train(more_sentences)</span><br></pre></td></tr></table></figure><h1 id="一些模型的用处"><a href="#一些模型的用处" class="headerlink" title="一些模型的用处"></a>一些模型的用处</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(positive=[&apos;woman&apos;, &apos;king&apos;], negative=[&apos;man&apos;], topn=1)</span><br><span class="line">[(&apos;queen&apos;, 0.50882536)]</span><br><span class="line">model.doesnt_match(&quot;breakfast cereal dinner lunch&quot;.split())</span><br><span class="line">&apos;cereal&apos;</span><br><span class="line">model.similarity(&apos;woman&apos;, &apos;man&apos;)</span><br><span class="line">0.73723527</span><br></pre></td></tr></table></figure><p>most_similar的用法：取最接近（positive向量和 - negative向量和）的前topn个词，并给出相似度</p><p>positive (<em>list of str</em>, <em>optional</em>) – List of words that contribute positively.</p><p>negative<em>(</em>list of str<em>,</em> <em>optional</em>) – List of words that contribute negatively.</p><p>topn (<em>int,</em> <em>optional</em>) – Number of top-N similar words to return.</p><p>similarity的用法：计算两个单词的余弦相似度</p><p>doesnt_match的用法：找到list里面与其他词最不一相似的那一个</p></the>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gensim </tag>
            
            <tag> word2vec </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何建立这个站点</title>
      <link href="/2018/11/07/%E5%BB%BA%E7%AB%8B%E8%BF%99%E4%B8%AA%E7%AB%99%E7%82%B9/"/>
      <url>/2018/11/07/%E5%BB%BA%E7%AB%8B%E8%BF%99%E4%B8%AA%E7%AB%99%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p><excerpt in="" index="" |="" 首页摘要="">摘要：这里简单记录一下这个站点建立的过程<a id="more"></a></excerpt></p><the rest="" of="" contents="" |="" 余下全文=""><h1 id="建立github-page"><a href="#建立github-page" class="headerlink" title="建立github page"></a>建立github page</h1><h2 id="申请一个github账号"><a href="#申请一个github账号" class="headerlink" title="申请一个github账号"></a>申请一个github账号</h2><p>进入 <a href="https://github.com/join?source=header-home" target="_blank" rel="noopener">github申请界面</a></p><p><img src="/2018/11/07/建立这个站点/github申请.png" alt="github申请"></p><h2 id="新建一个仓库"><a href="#新建一个仓库" class="headerlink" title="新建一个仓库"></a>新建一个仓库</h2><p><img src="/2018/11/07/建立这个站点/git建仓库.png" alt="git建仓库"></p><p><strong>注意：仓库名需要与你的github用户名一致</strong></p><p>这时，github page就自动生成了。地址是：https://{仓库名}.github.io</p><h1 id="使用hexo建立站点"><a href="#使用hexo建立站点" class="headerlink" title="使用hexo建立站点"></a>使用hexo建立站点</h1><p>需要环境：npm等</p><h2 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><h2 id="创建本地文件夹，用来放博客文件"><a href="#创建本地文件夹，用来放博客文件" class="headerlink" title="创建本地文件夹，用来放博客文件"></a>创建本地文件夹，用来放博客文件</h2><h2 id="进入该文件夹的powershell界面"><a href="#进入该文件夹的powershell界面" class="headerlink" title="进入该文件夹的powershell界面"></a>进入该文件夹的powershell界面</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>会生成一批文件</p><p><img src="/2018/11/07/建立这个站点/hexo生成的文件" alt="hexo生成的文件.png"> </p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>_config.yml文件是配置文件，主要的配置项可以参考<a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">hexo官方文档</a></p><p>一定要配置的有</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: https://github.com/&#123;仓库名&#125;/&#123;仓库名&#125;.github.io.git</span><br><span class="line">branch: gh-pages</span><br></pre></td></tr></table></figure><h2 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line"></span><br><span class="line">hexo generate</span><br><span class="line"></span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><p>此时可以打开页面：https://{仓库名}.github.io 查看博客详情</p><h1 id="使用主题"><a href="#使用主题" class="headerlink" title="使用主题"></a>使用主题</h1><h2 id="下载主题"><a href="#下载主题" class="headerlink" title="下载主题"></a>下载主题</h2><p>本博客使用的是yelee主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/MOxFIVE/hexo-theme-yelee.git themes/yelee</span><br></pre></td></tr></table></figure><p>下载完成后，会在项目themes目录下生成yelle文件夹 </p><p>修改项目根目录配置文件_config.yml，即可切换至Yelle主题 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theme: Yelee</span><br></pre></td></tr></table></figure><p><img src="/2018/11/07/建立这个站点/修改主题.png" alt="img"> </p><h2 id="主题预览"><a href="#主题预览" class="headerlink" title="主题预览"></a>主题预览</h2><p>执行以下命令预览主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo s</span><br></pre></td></tr></table></figure><p>yelee主题的诸多配置可以参考该说明 <a href="http://moxfive.coding.me/yelee/1.Getting-Started/" target="_blank" rel="noopener">yelee主题使用说明</a></p></the>]]></content>
      
      
      <categories>
          
          <category> tools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
